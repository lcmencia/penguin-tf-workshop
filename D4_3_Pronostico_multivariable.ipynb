{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/torresmateo/penguin-tf-workshop/blob/master/D4_3_Pronostico_multivariable.ipynb\" target=\"_parent\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Pronóstico multivariable\n",
    "\n",
    "En este notebook, extendemos el poder predictivo de nuestro modelo de series temporales incluyendo más variables en el \n",
    "entrenamiento.\n",
    "\n",
    "Primero que nada, importamos las bibliotecas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "blMU3F_ZCgjV",
    "outputId": "98031285-b386-4bff-c34b-77b9295971ef"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# importamos Pandas, que nos permitirá ver mejor los datos\n",
    "import pandas as pd\n",
    "\n",
    "# configuramos las figuras de matplotlib\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6qhWZwSCzE3"
   },
   "source": [
    "Bajamos los datos, y definimos la función que nos ayudará a visualizar las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5B9z9a2rCzle"
   },
   "outputs": [],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()\n",
    "\n",
    "def ver_ejemplo(ejemplo, delta, titulos):\n",
    "    rotulos = ['Historia', 'Futuro verdadero'] + titulos[1:]\n",
    "    marker = ['.-', 'rx', 'go', 'mD', 'c<']\n",
    "    tiempos = list(range(-ejemplo[0].shape[0], 0))\n",
    "    if delta:\n",
    "        futuro = delta\n",
    "    else:\n",
    "        futuro = 0\n",
    "    \n",
    "    plt.title(titulos[0])\n",
    "    for i, x in enumerate(ejemplo):\n",
    "        if i:  # futuro verdadero o predicción\n",
    "            plt.plot(futuro, ejemplo[i], marker[i], markersize=10, label=rotulos[i])\n",
    "        else:  # historia\n",
    "            plt.plot(tiempos, ejemplo[i], marker[i], label=rotulos[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([tiempos[0], (futuro+5)*2])\n",
    "    plt.xlabel('Tiempo')\n",
    "    return plt\n",
    "\n",
    "def ver_ejemplo_multivalor(historia, futuro_real, prediccion):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tiempos_historia = list(range(-len(historia), 0))\n",
    "    futuro = len(futuro_real)\n",
    "\n",
    "    plt.plot(tiempos_historia, np.array(historia[:, 1]), label='Historia')\n",
    "    plt.plot(np.arange(futuro)/6, np.array(futuro_real), 'bo',\n",
    "            label='Futuro Real')\n",
    "    if prediccion.any():\n",
    "        plt.plot(np.arange(futuro)/6, np.array(prediccion), 'ro',\n",
    "                label='Prediccion')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3I0_Uv6C5P-"
   },
   "source": [
    "# Preparar un *dataset* multivariable\n",
    "\n",
    "Si bien el dataset original es de 14 variables, por simplicidad consideremos 3:\n",
    "* temperatura\n",
    "* presión atmosférica\n",
    "* densidad del aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEJuO_O6DI33"
   },
   "outputs": [],
   "source": [
    "lista_features = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZWBuiqcDNKr"
   },
   "source": [
    "filtramos el *dataset*, dejando solamente las columnas correspondientes a nuestra lista de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "GnOGHmqEDVz5",
    "outputId": "63f90040-54df-499c-8f48-203eaa9e1e1f"
   },
   "outputs": [],
   "source": [
    "features = df[lista_features]\n",
    "features.index = df['Date Time']\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEdmn_ypDfiN"
   },
   "source": [
    "exploremos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "gKE5sqncDkv2",
    "outputId": "e00d31cb-3726-4401-b059-03dd038aaa98"
   },
   "outputs": [],
   "source": [
    "features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaYTiY3qDuT2"
   },
   "source": [
    "# Particion y normalización del *dataset*\n",
    "\n",
    "Al igual que el caso univariable, debemos normalizar solo considerando la media y desviación estándar del *training set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb_crHPuD4lx"
   },
   "outputs": [],
   "source": [
    "particion = 300000\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# por ahora solo extraemos el training set, que se usa para calcular los parámetros para normalizar\n",
    "training_data = features[:particion] \n",
    "scaler.fit(training_data)\n",
    "\n",
    "# normalizamos todo el dataset\n",
    "features = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xi2f1b8kEGp_"
   },
   "source": [
    "# Preparamos los ejemplos para una predicción multivariable\n",
    "\n",
    "Esta función sirve para procesar el *dataset* y generar ejemplos con los *inputs* y *labels* que luego se usarán para el entrenamiento y validación de nuestro modelo. De forma similar al ejemplo univariale, consideraremos una ventana del pasado para predecir un punto en el futuro. Sin embargo, también incluimos la opción de usar varios puntos futuros como *label*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btm58n30EkWs"
   },
   "outputs": [],
   "source": [
    "def ejemplos_multivariable(dataset, target, inicio, fin, ancho_ventana, offset_prediccion, paso, multi_valor = False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    inicio = inicio + ancho_ventana\n",
    "    if fin is None:\n",
    "        fin = len(dataset) - offset_prediccion\n",
    "    \n",
    "    for i in range(inicio, fin):\n",
    "        # el paso nos permite tomar diferentes resoluciones de muestra.\n",
    "        # En este caso, el dataset tiene una muestra cada 10 minutos.\n",
    "        # Si quisieramos, por ejemplo, una muestra por hora, el paso es 6.\n",
    "        # Definimos nuestra ventana tomando eso en cuenta\n",
    "        ventana = range(i-ancho_ventana, i, paso)\n",
    "        \n",
    "        # agregamos la muestras a nuestro ejemplo\n",
    "        data.append(dataset[ventana])\n",
    "\n",
    "        # agregamos el label\n",
    "        if multi_valor:\n",
    "            # en caso de que queramos predecir muchos valores, agregamos todas\n",
    "            # las muestras que estan después de la ventana, hasta el límite de\n",
    "            # predicción\n",
    "            labels.append(target[i:i+offset_prediccion])\n",
    "        else:\n",
    "            # si predecimos un solo valor, hacemos lo mismo que en el ejemplo anterior, y \n",
    "            # solo agregamos la muestra en el indice ubicado `prediccion` lugares luego de \n",
    "            # la ventana\n",
    "            labels.append(target[i+offset_prediccion])\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktiqXhdtHNmx"
   },
   "source": [
    "con esa función, podemos hacer la partición de nuestro dataset en *training* y *testing*. Queremos que nuestro modelo vea datos de los útimos 5 días, es decir 720 muestras (5 x 24 x 6) y queremos predecir la temperatura 12 horas en el futuro, luego de 72 muestras (12 x 6).\n",
    "\n",
    "El `target` es la temperatura, y por ello le pasamos solo la segunda columna del dataset (`features[:,1]`). También cambiamos la resolución de las muestras a una muestra por hora, por lo que establecemos el `paso` a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwz23oLAHTpb"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = ejemplos_multivariable(features, features[:, 1], 0, particion, 720, 72, 6)\n",
    "x_test, y_test = ejemplos_multivariable(features, features[:, 1], particion, None, 720, 72, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "veapls-bJDIA"
   },
   "source": [
    "Igual al ejemplo anterior, preparamos el dataset usando `tf.data` partiendo el dataset en *batches* con *shuffling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRW1AIJZHmP6"
   },
   "outputs": [],
   "source": [
    "batch = 256\n",
    "buffer = 10000\n",
    "\n",
    "# creamos el training dataset\n",
    "train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train = train.cache().shuffle(buffer).batch(batch).repeat()\n",
    "\n",
    "# creamos el testing dataset\n",
    "test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test = test.batch(batch).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pf0HX3WdJx-E"
   },
   "source": [
    "# Creamos la red neuronal\n",
    "\n",
    "Podemos usar la misma red que en el caso anterior. Observar que la dimensionalidad de nuestros datos es diferente, por lo que automáticamente el *layer* con unidades LSTM se adaptará cuando usamos `shape[-2:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPxGL2qpKDug"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.LSTM(8, input_shape=x_train.shape[-2:]),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xh_CiWDKKKYg"
   },
   "source": [
    "entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "MYWqMVSRKL8s",
    "outputId": "0cb0ee31-26fa-49a3-ceff-1f73dc4d463c"
   },
   "outputs": [],
   "source": [
    "model.fit(train, epochs=10, steps_per_epoch=200, validation_data=test, validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvDi6XwEKWcj"
   },
   "source": [
    "y vemos unas cuantas predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NcCiECb2KY-6",
    "outputId": "924bad6a-be0a-426f-9a08-bcd42b6f5717"
   },
   "outputs": [],
   "source": [
    "for x,y in test.take(4):\n",
    "    ver_ejemplo([x[0][:,1].numpy() ,y[0].numpy(), model.predict(x)[0]], 12, \n",
    "                ['Comparación de modelos', 'shallow LSTM'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ol7-mk6iLwu1"
   },
   "source": [
    "# Extrapolación de muchos valores\n",
    "\n",
    "En este caso, queremos usar la misma información de entrada, pero en lugar de predecir la temperatura luego de 12 horas, queremos predecir la temperatura para cada una de las siguientes 12 horas.\n",
    "\n",
    "Los parámetros son similares, con la única diferencia que necesitamos es indicarle a nuestro generador de ejemplos que los *labels* son multi valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4__3slOmMd5b"
   },
   "outputs": [],
   "source": [
    "x_train_multi, y_train_multi = ejemplos_multivariable(features, features[:, 1], 0, particion, 720, 72, 6, multi_valor=True)\n",
    "x_test_multi, y_test_multi = ejemplos_multivariable(features, features[:, 1], particion, None, 720, 72, 6, multi_valor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MJOu_PRWNLsz",
    "outputId": "379ad0a4-7576-4cc4-d95e-f7204a2cfbcd"
   },
   "outputs": [],
   "source": [
    "x_train_multi.shape, y_train_multi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVW4XCSJMvAk"
   },
   "source": [
    "y lo pre-procesamos con `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bylMpPJMx7U"
   },
   "outputs": [],
   "source": [
    "batch = 256\n",
    "buffer = 10000\n",
    "\n",
    "# creamos el training dataset\n",
    "train_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_multi = train_multi.cache().shuffle(buffer).batch(batch).repeat()\n",
    "\n",
    "# creamos el testing dataset\n",
    "test_multi = tf.data.Dataset.from_tensor_slices((x_test_multi, y_test_multi))\n",
    "test_multi = test_multi.batch(batch).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGsSgeONMpRS"
   },
   "source": [
    "veamos un ejemplo del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "KhEM4QL1MsPP",
    "outputId": "5a8d0cb8-d1ce-4a62-8b19-1dbfb1014f25"
   },
   "outputs": [],
   "source": [
    "\n",
    "for x, y in train_multi.take(1):\n",
    "    ver_ejemplo_multivalor(x[0], y[0], np.array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmP-6xkNOn3V"
   },
   "source": [
    "# Defninimos la red neuronal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTiZqhl0Oqsy"
   },
   "outputs": [],
   "source": [
    "model_multi = tf.keras.models.Sequential([tf.keras.layers.LSTM(32, return_sequences=True, input_shape=x_train.shape[-2:]),\n",
    "                                          tf.keras.layers.LSTM(16, activation='relu'),\n",
    "                                          tf.keras.layers.Dense(72)])\n",
    "\n",
    "model_multi.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lq0k1pwVPejv"
   },
   "source": [
    "entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "EV6wtvqtPflM",
    "outputId": "73600355-4b47-4d20-ccc9-9ce318ee5865"
   },
   "outputs": [],
   "source": [
    "model_multi.fit(train_multi, epochs=10, steps_per_epoch=200, validation_data=test_multi, validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpDwLjF8Pk4S"
   },
   "source": [
    "vemos unas cuantas predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8icW4qs_PmZ_"
   },
   "outputs": [],
   "source": [
    "for x, y in test_multi.take(3):\n",
    "    ver_ejemplo_multivalor(x[0], y[0], model_multi.predict(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYlvNJGgCr0d"
   },
   "source": [
    "# Créditos\n",
    "\n",
    "Este notebook traduce y adapta el código y explicaciones del [Tutorial de Tensorflow](https://www.tensorflow.org/tutorials/structured_data/time_series) en series temporales."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "D4_3_Pronostico_multivariable.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
